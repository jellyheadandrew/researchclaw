"""
summarizer.py â€” Experiment summarization and REPORT.md generation.

Responsibilities:
  1. Format Slack status messages for experiment events (FINISHED, CRASHED, etc.)
  2. Parse numeric metrics from experiment log output
  3. Generate a full REPORT.md when a trial completes
  4. Use the LLM to write observations and suggested next directions

All file writes go through PathValidator.
"""

from __future__ import annotations

import logging
from datetime import datetime
from pathlib import Path

from .access_control import PathValidator
from .llm import LLMProvider
from .models import TrialInfo
from .utils import parse_metrics_from_log, tail_file
from .watcher import ExperimentEvent, ExperimentStatus

logger = logging.getLogger("researchclaw.summarizer")

REPORT_TEMPLATE = """\
# Trial Report: {trial_name}
**Date:** {date}
**Started:** {started_at}
**Finished:** {finished_at}
**Status:** {status}

## Research Goal
{goal}

## Code Changes
{diff_summary}

### Files Modified
{file_list}

## Experiment Results

### Runs
{run_table}

### Key Metrics
{metrics_summary}

### Best Result
{best_result}

## Observations
{observations}

## Suggested Next Directions
{suggestions}

---
*Generated by ResearchClaw at {generated_at}*
"""

REPORT_ANALYSIS_PROMPT = """\
You are analyzing the results of an ML experiment trial.

Code changes made:
{diff}

Experiment metrics:
{metrics}

Log excerpts (last {log_lines} lines):
{log_tail}

Write two sections:
1. OBSERVATIONS: What happened? Did the changes help? Any anomalies?
   Be specific. Reference actual numbers.
2. SUGGESTED NEXT DIRECTIONS: Based on these results, what should be tried next?
   Give 2-3 concrete, actionable suggestions. Not vague â€” specific
   hyperparameters, architectural changes, or debugging steps.

Format your response as:
OBSERVATIONS:
<your text>

SUGGESTED NEXT DIRECTIONS:
<your text>
"""


class Summarizer:
    """Generate experiment summaries and REPORT.md files."""

    def __init__(
        self,
        llm: LLMProvider,
        validator: PathValidator,
        base_dir: str,
        log_tail_lines: int = 50,
    ):
        self.llm = llm
        self.validator = validator
        self.base_dir = Path(base_dir).resolve()
        self.log_tail_lines = log_tail_lines

    def format_status_message(self, status: ExperimentStatus) -> str:
        """Format a Slack message for an experiment event."""
        if status.event == ExperimentEvent.FINISHED:
            gpu = status.gpu_info
            gpu_str = (
                f"GPU: {gpu.get('utilization', 0):.0f}% util, "
                f"{gpu.get('memory_used_mb', 0):.0f}/{gpu.get('memory_total_mb', 0):.0f}MB"
                if gpu else "GPU: not available"
            )
            files_str = ""
            if status.new_files:
                files_str = "\nâ”œ Output files: " + ", ".join(
                    Path(f).name for f in status.new_files[:5]
                )

            return (
                f"âœ… {status.trial_name} finished\n"
                f"â”œ Runtime: {status.duration}\n"
                f"â”œ {gpu_str}"
                f"{files_str}\n"
                f"\nFinal log:\n```\n{status.log_tail[-500:]}\n```"
            )

        elif status.event == ExperimentEvent.CRASHED:
            return (
                f"âŒ {status.trial_name} CRASHED (exit code {status.returncode})\n"
                f"â”œ Runtime: {status.duration}\n"
                f"\nLast output:\n```\n{status.log_tail[-500:]}\n```"
            )

        elif status.event == ExperimentEvent.HUNG:
            return (
                f"âš ï¸  {status.trial_name}: {status.message}\n"
                f"â”œ Runtime: {status.duration}\n"
                f"\nLast output:\n```\n{status.log_tail[-300:]}\n```\n"
                f"Should I kill it? [Y/N]"
            )

        elif status.event == ExperimentEvent.NAN_DETECTED:
            return (
                f"âš ï¸  {status.trial_name}: {status.message}\n"
                f"â”œ Runtime: {status.duration}\n"
                f"\nLast output:\n```\n{status.log_tail[-300:]}\n```\n"
                f"Should I stop the experiment? [Y/N]"
            )

        elif status.event == ExperimentEvent.STATUS_UPDATE:
            gpu = status.gpu_info
            gpu_str = (
                f"GPU {gpu.get('utilization', 0):.0f}%"
                if gpu else "GPU: N/A"
            )
            return (
                f"ðŸ“Š {status.trial_name} running â€” {status.duration} elapsed ({gpu_str})\n"
                f"```\n{status.log_tail[-300:]}\n```"
            )

        return f"[{status.event.value}] {status.trial_name} â€” {status.duration}"

    def generate_report(
        self,
        trial: TrialInfo,
        diff: str,
        extra_context: dict | None = None,
    ) -> str:
        """
        Generate a full REPORT.md for a completed trial.
        Writes the report to experiment_reports/{date}/trial_{N}/REPORT.md.
        Returns the report text.
        """
        log_dir = self.base_dir / trial.report_path / "log"
        stdout_log = log_dir / "stdout.log"

        log_text = tail_file(str(stdout_log), n=200) if stdout_log.exists() else ""
        log_tail = tail_file(str(stdout_log), n=self.log_tail_lines) if stdout_log.exists() else ""
        metrics = self.parse_metrics(log_text)

        observations, suggestions = self._llm_analyze(diff, metrics, log_tail)

        # Build metrics summary
        if metrics:
            metrics_summary = "\n".join(f"- {k}: {v:.4f}" for k, v in metrics.items())
            best_result = max(metrics.items(), key=lambda x: x[1], default=("(none)", 0))
            best_result_str = f"{best_result[0]} = {best_result[1]:.4f}"
        else:
            metrics_summary = "(no metrics parsed from log)"
            best_result_str = "(no metrics available)"

        # Scan for output files
        sandbox = self.base_dir / trial.sandbox_path
        notable_files = []
        if sandbox.exists():
            for ext in (".pt", ".ckpt", ".png", ".pdf", ".csv"):
                notable_files.extend(
                    str(p.relative_to(self.base_dir)) for p in sandbox.rglob(f"*{ext}")
                )

        run_table = f"| Trial | Status | Duration |\n|---|---|---|\n| {trial.trial_name} | {trial.status} | see log |"

        report = REPORT_TEMPLATE.format(
            trial_name=trial.trial_name,
            date=trial.date,
            started_at=trial.started_at,
            finished_at=trial.finished_at or "(in progress)",
            status=trial.status,
            goal=trial.goal or "(not specified)",
            diff_summary=diff[:3000] if diff else "(no diff available)",
            file_list="\n".join(f"- {f}" for f in notable_files) or "(no notable files)",
            run_table=run_table,
            metrics_summary=metrics_summary,
            best_result=best_result_str,
            observations=observations,
            suggestions=suggestions,
            generated_at=datetime.now().isoformat(),
        )

        # Write report through PathValidator
        report_path_abs = self.base_dir / trial.report_path / "REPORT.md"
        validated_path = self.validator.validate_write(str(report_path_abs))
        validated_path.write_text(report)
        logger.info("REPORT.md written to %s", validated_path)

        return report

    def parse_metrics(self, log_text: str) -> dict[str, float]:
        """Extract numeric metrics from log output."""
        return parse_metrics_from_log(log_text)

    # ------------------------------------------------------------------
    # RESEARCH_TRIAL_SUMMARY.md entry generation
    # ------------------------------------------------------------------

    TRIAL_SUMMARY_PROMPT = (
        "Summarize this completed experiment trial concisely.\n"
        "Include:\n"
        "1. Key result (most important metric or outcome, with numbers if available)\n"
        "2. Lessons learned (what worked, what didn't, what to try differently)\n\n"
        "Trial: {trial_name} ({date})\n"
        "Goal: {goal}\n"
        "Status: {status}\n\n"
        "Report content:\n{report_excerpt}\n\n"
        "Format your response EXACTLY as:\n"
        "KEY_RESULT: <one line>\n"
        "LESSONS_LEARNED: <2-3 lines>\n"
    )

    def generate_trial_summary_entry(
        self,
        trial: TrialInfo,
        commit_hash: str | None = None,
    ) -> str:
        """Generate a markdown entry for RESEARCH_TRIAL_SUMMARY.md.

        Reads the trial's REPORT.md and uses the LLM to extract a concise
        key result and lessons learned.  Returns a formatted markdown block.
        """
        report_path = self.base_dir / trial.report_path / "REPORT.md"
        report_text = report_path.read_text(errors="replace") if report_path.exists() else "(no report generated)"

        # Use LLM to distill the report
        try:
            response = self.llm.complete(
                system_prompt="You are a precise ML research analyst. Be specific and concise.",
                user_message=self.TRIAL_SUMMARY_PROMPT.format(
                    trial_name=trial.trial_name,
                    date=trial.date,
                    goal=trial.goal or "(not specified)",
                    status=str(trial.status).upper(),
                    report_excerpt=report_text[:3000],
                ),
                max_tokens=256,
            )
            key_result, lessons = self._parse_trial_summary_response(response)
        except Exception as e:
            logger.warning("LLM trial summary generation failed: %s", e)
            key_result = "(summary generation failed â€” see REPORT.md)"
            lessons = "(see REPORT.md for details)"

        outcome = (
            f"Approved and merged (commit {commit_hash})"
            if commit_hash
            else trial.status.capitalize() if isinstance(trial.status, str) else trial.status.value.capitalize()
        )

        status_label = str(trial.status).upper() if isinstance(trial.status, str) else trial.status.value.upper()

        return (
            f"### {trial.trial_name} ({trial.date}) -- {status_label}\n"
            f"**Goal:** {trial.goal or '(not specified)'}\n"
            f"**Key result:** {key_result}\n"
            f"**Outcome:** {outcome}\n"
            f"**Lessons learned:** {lessons}\n"
        )

    @staticmethod
    def _parse_trial_summary_response(response: str) -> tuple[str, str]:
        """Parse KEY_RESULT and LESSONS_LEARNED from LLM response."""
        key_result = "(not generated)"
        lessons = "(not generated)"

        if "KEY_RESULT:" in response:
            parts = response.split("KEY_RESULT:", 1)
            rest = parts[1].strip()
            if "LESSONS_LEARNED:" in rest:
                kr_part, ll_part = rest.split("LESSONS_LEARNED:", 1)
                key_result = kr_part.strip()
                lessons = ll_part.strip()
            else:
                key_result = rest.split("\n", 1)[0].strip()

        return key_result, lessons

    def _llm_analyze(
        self,
        diff: str,
        metrics: dict[str, float],
        log_tail: str,
    ) -> tuple[str, str]:
        """
        Use the LLM to generate observations and suggested next directions.
        Returns (observations_text, suggestions_text).
        """
        metrics_str = "\n".join(f"{k}: {v:.4f}" for k, v in metrics.items()) or "(none parsed)"
        prompt = REPORT_ANALYSIS_PROMPT.format(
            diff=diff[:2000] if diff else "(no diff)",
            metrics=metrics_str,
            log_lines=self.log_tail_lines,
            log_tail=log_tail[-1500:] if log_tail else "(no log)",
        )

        try:
            response = self.llm.complete(
                system_prompt="You are a precise ML research analyst. Be specific and concise.",
                user_message=prompt,
                max_tokens=1024,
            )
        except Exception as e:
            logger.error("LLM analysis failed: %s", e)
            return "(LLM analysis failed â€” see raw log)", "(unavailable)"

        # Parse OBSERVATIONS / SUGGESTED NEXT DIRECTIONS sections
        observations = "(not generated)"
        suggestions = "(not generated)"

        if "OBSERVATIONS:" in response:
            parts = response.split("OBSERVATIONS:", 1)
            rest = parts[1].strip()
            if "SUGGESTED NEXT DIRECTIONS:" in rest:
                obs_part, sug_part = rest.split("SUGGESTED NEXT DIRECTIONS:", 1)
                observations = obs_part.strip()
                suggestions = sug_part.strip()
            else:
                observations = rest.strip()

        return observations, suggestions
